{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import climtas\n",
    "import xarray\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style=\"border: 2px solid white;\">\n",
       "<tr>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Client</h3>\n",
       "<ul style=\"text-align: left; list-style: none; margin: 0; padding: 0;\">\n",
       "  <li><b>Scheduler: </b>tcp://127.0.0.1:42675</li>\n",
       "  <li><b>Dashboard: </b><a href='http://127.0.0.1:8787/status' target='_blank'>http://127.0.0.1:8787/status</a>\n",
       "</ul>\n",
       "</td>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Cluster</h3>\n",
       "<ul style=\"text-align: left; list-style:none; margin: 0; padding: 0;\">\n",
       "  <li><b>Workers: </b>1</li>\n",
       "  <li><b>Cores: </b>1</li>\n",
       "  <li><b>Memory: </b>4.00 GB</li>\n",
       "</ul>\n",
       "</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<Client: 'tcp://127.0.0.1:42675' processes=1 threads=1, memory=4.00 GB>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import dask.distributed\n",
    "\n",
    "# Edit as desired\n",
    "threads_per_worker = 1\n",
    "\n",
    "try:\n",
    "    c # Already running\n",
    "except NameError:\n",
    "    c = dask.distributed.Client(\n",
    "        n_workers=int(os.environ.get('PBS_NCPUS', 1))//threads_per_worker,\n",
    "        threads_per_worker=threads_per_worker,\n",
    "        memory_limit=f'{4*threads_per_worker}gb',\n",
    "        local_directory=os.path.join(os.environ.get('PBS_JOBFS'),'dask-worker-space')\n",
    "    )\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the dataset - The initial size is 1.5 TB\n",
    "\n",
    "# We're starting out with the same latitude and longitude chunking as are in the file, chunking\n",
    "# along the time axis defaults to the file size, so one month in this case\n",
    "\n",
    "ds = xarray.open_mfdataset(sorted(glob('/g/data/ub4/era5/netcdf/surface/2T/*/2T_era5_global_*.nc')),\n",
    "                           combine='nested',\n",
    "                           concat_dim='time',\n",
    "                           chunks={'latitude': 91, 'longitude': 180})\n",
    "t2m = ds.t2m\n",
    "\n",
    "print(\"File chunking:\", dict(zip(t2m.dims, t2m.encoding['chunksizes'])))\n",
    "\n",
    "t2m.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to daily mean - or max, min etc. This reduces the size to 62 GB, with the same number of chunks\n",
    "import climtas.blocked\n",
    "\n",
    "t2m_daily = climtas.blocked.blocked_resample(t2m, time=24).mean()\n",
    "t2m_daily.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Smooth out the data\n",
    "# We also increase the chunking along the time dimension at this point - too small chunks makes the 'Tasks' count increase rapidly\n",
    "\n",
    "t2m_smooth = t2m_daily.chunk({'time': 600}).rolling(time=15, center=True).mean()\n",
    "t2m_smooth.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate percentiles\n",
    "\n",
    "# We've done the processing so far on a wider time range than we need, so that the\n",
    "# rolling operation doesn't start right at our analysis start date. Now's the time to\n",
    "# select just the dates we need\n",
    "\n",
    "t2m_percentile = climtas.apply_doy.percentile_doy(t2m_smooth.sel(time=slice('1980','2018')), 90)\n",
    "t2m_percentile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# At this point we've reduced our 1.5 TB of data to 1.5 GB - time to save it to a file\n",
    "\n",
    "t2m_percentile.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the throttled saver to write to netcdf one chunk at a time, so that memory doesn't get filled up\n",
    "\n",
    "climtas.io.to_netcdf_throttled(t2m_percentile.to_dataset(name='t2m_percentile'),\n",
    "                               '/g/data/w35/saw562/era5_heatwave_clim.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the climatology back out of the file\n",
    "\n",
    "threshold = xarray.open_dataset('/g/data/w35/saw562/era5_heatwave_clim.nc',\n",
    "                           chunks={'latitude': 200, 'longitude': 200}).t2m_percentile\n",
    "threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "threshold.sel(latitude=-37.8, longitude=144.9, method='nearest').plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t2m_daily.groupby('time.dayofyear') > threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "climtas.event.find_events(t2m_daily.groupby('time.dayofyear') > threshold, min_duration=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t2m_daily.groupby('time.dayofyear') > threshold.transpose('dayofyear','latitude','longitude')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
